# ğŸ§  RAG Cortex

A powerful Retrieval-Augmented Generation (RAG) chatbot that lets you chat with your PDF documents. Built with **Streamlit**, **LangChain**, **Groq**, and **ChromaDB**.

## âœ¨ Features
- **Chat with PDFs** â€” Ask questions and get accurate answers based on your documents
- **Dark Notion-style UI** â€” Clean, minimal dark theme interface
- **Document Management** â€” Add and delete documents from the sidebar
- **Duplicate Detection** â€” Warns before re-uploading files already indexed
- **Persistent Memory** â€” ChromaDB saves embeddings to disk (load in seconds)
- **High-Performance LLM** â€” Groq API running Llama 3.3 70B
- **Local Embeddings** â€” Ollama `nomic-embed-text` for private processing

## ğŸ› ï¸ Tech Stack
| Component | Tool | Why? |
|-----------|------|------|
| **Frontend** | Streamlit | Fast, interactive UI in pure Python |
| **Framework** | LangChain | Orchestrates the RAG pipeline |
| **LLM** | Groq API | Extremely fast inference for Llama 3 |
| **Embeddings** | Ollama | Runs `nomic-embed-text` locally |
| **Vector Store** | ChromaDB | Persists to disk (unlike RAM-only FAISS) |

## ğŸ“ Project Structure
```
RAG-Chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py          # Streamlit UI
â”‚   â”œâ”€â”€ rag_core.py     # RAG logic
â”‚   â””â”€â”€ styles.css      # Dark theme CSS
â”œâ”€â”€ documents/          # Your PDFs go here
â”œâ”€â”€ rag_vector_store/   # ChromaDB persistence
â””â”€â”€ .env                # API keys
```

## âš™ï¸ Setup

1. **Prerequisites**:
   - Python 3.13+
   - [Ollama](https://ollama.com/) installed
   - Pull the embedding model: `ollama pull nomic-embed-text`

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Environment Variables**:
   Create a `.env` file in the root directory:
   ```env
   GROQ_API_KEY=your_groq_api_key_here
   ```

4. **Run the App**:
   ```bash
   cd src
   streamlit run app.py
   ```

## ğŸ§  How It Works
1. **Ingestion** â€” Scans `documents/` folder for PDFs
2. **Chunking** â€” Splits text into 1000-char chunks (200 overlap)
3. **Embedding** â€” Converts text to vectors via `nomic-embed-text`
4. **Storage** â€” Saves vectors to `rag_vector_store/` (ChromaDB)
5. **Retrieval** â€” Finds most similar chunks for your question
6. **Generation** â€” Sends question + context to Groq (Llama 3.3)

## ğŸ’¡ Lessons Learned

### Vector Store: FAISS vs Chroma
- **FAISS**: Stores in RAM, requires re-processing on restart
- **Chroma**: Persists to disk, instant 2-second reload âœ“

### Embeddings: Speed vs Accuracy
- **HuggingFace** (`all-MiniLM-L6-v2`): Fast but lower accuracy
- **Ollama** (`nomic-embed-text`): Best balance, 8192 token context âœ“
- **FastEmbed** (`BAAI/bge-small`): Future option for 1000+ docs
